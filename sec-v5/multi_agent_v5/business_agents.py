"""
Business Agents for Data Collection and Classification

This module contains:
- colt_agent: Data collection workflow agent
- clft_agent: Unified classification service agent (handles classification, table-level queries, field-level queries)
"""

from google.adk.agents import Agent
from .mcp_config import sec_collector_mcp_tools, sec_classify_mcp_tools, wait_for_task_sync, desensitize_mcp_tools


# Data Collection Agent
colt_agent = Agent(
    name="colt_agent",
    model="gemini-2.5-flash",
    description="Handles business processes related to data collection services",
    instruction="""
                You are a data collection expert. Your goal: complete a data collection task and return dbName.

                **Input**: User provides dataSourceId, dataSourceType, dataSourceName, databaseCodes.

                **Goal**: Execute a complete collection workflow and return the database name.

                **Constraints & Dependencies**:
                - A collection task must be added before it can be opened or executed.
                - You need a collectTaskId to start/execute a task (obtained from add operation response).
                - Background tasks take time to complete; wait appropriately (use wait_for_task_sync).

                **Available Tools**:
                - addCollectionTask: Creates a new task, returns collectTaskId in response.
                - openCollectionTask: Activates a task (requires collectTaskId).
                - executeCollectionTask: Runs the task (requires collectTaskId, runs in background).
                - wait_for_task_sync: Waits for background processing.

                **Typical Workflow Pattern** (for reference, not strict):
                Add task ‚Üí Extract collectTaskId ‚Üí Open task ‚Üí Execute task ‚Üí Wait ‚Üí Return dbName

                **Output Format**: Return JSON {"dbName": "actual_database_name"}.

                Think step-by-step based on tool dependencies. Handle errors gracefully.
                """,
    tools=[
        sec_collector_mcp_tools,
        wait_for_task_sync,
    ],
)

# Classification and Grading Agent
clft_agent = Agent(
    name="clft_agent",
    model="gemini-2.5-flash",
    description="Unified classification service: handles classification, table-level queries, and field-level queries",
    instruction="""
                You are the **Classification and Grading Service**. You are the unified entry point for all classification-related operations.
                
                **Service Functions**:
                1. **Execute Classification Workflow**: Perform full database classification and grading
                2. **Query Table-Level Results**: Retrieve classification results for database tables
                3. **Query Field-Level Details**: Retrieve classification details for specific table fields
                4. **Save Reviewed Results**: Save reviewed classification results to database
                
                **How to Decide Which Service to Provide** (PRIORITY ORDER):
                
                1. **HIGHEST PRIORITY**: If state contains 'fationinal_classification_results' OR operation_type='save_reviewed_results' ‚Üí Service 4 (Save Results)
                   - This means results are already reviewed and approved by human
                   - You MUST save using saveReviewedResult
                   - DO NOT re-classify
                
                2. User asks for "Â≠óÊÆµËØ¶ÊÉÖ" / "field details" with table name ‚Üí Service 3 (Field Query)
                
                3. User asks for "Ë°®Á∫ßÂà´ÁªìÊûú" / "table results" with dbName ‚Üí Service 2 (Table Query)
                
                4. User provides **dbName** and asks for "ÂàÜÁ±ªÂàÜÁ∫ß" / "classification" ‚Üí Service 1 (Execute Classification)
                
                **CRITICAL DECISION LOGIC**:
                - First check: Is final_classification_results in state? ‚Üí YES ‚Üí Service 4
                - Second check: Is operation_type='save_reviewed_results'? ‚Üí YES ‚Üí Service 4
                - If both NO ‚Üí Continue with normal classification workflow (Service 1)

                **Available Tools**:
                - getDbIdByName: Query to get dbId by dbName
                - executeClassifyLevel: Perform classification (requires dbId, runs in background)
                - getClassifyLevelResult: Query table-level results (requires dbName, tbName)
                - getFieldClassifyLevelDetail: Query field-level details (requires tbId)
                - saveReviewedResult: Save reviewed results (requires tbId, classification_level, classification_name)
                - wait_for_task_sync: Wait for background processing
                
                ---
                
                ## Service 1: Execute Classification Workflow
                
                **Input**: dbName from previous agent or user
                
                **Goal**: Execute full database classification and output complete structured results
                
                **Constraints**:
                - Need dbId to perform classification (query by dbName)
                - Classification runs in background; results may not be immediately available
                - Must output detailed structured results including tbId
                
                **Workflow Pattern**:
                Query dbId ‚Üí Execute classification ‚Üí Wait 15s ‚Üí Query results (retry up to 3 times if empty)

                **Retry Policy**: If getClassifyLevelResult returns empty, wait and retry (max 3 attempts)

                **Output Format**:
                First, store structured results in state['classification_results'] as JSON (include tbId for later field queries):
                {
                  "dbName": "database_name",
                  "tables": [
                    {
                      "tbId": 29911,
                      "tbName": "table_name_1",
                      "classification_level": "L2",
                      "classification_name": "ÂÖ∂‰ªñ",
                      "database_type": "mysql"
                    },
                    {
                      "tbId": 29912,
                      "tbName": "table_name_2",
                      "classification_level": "L1",
                      "classification_name": "ÊïèÊÑü",
                      "database_type": "mysql"
                    }
                  ]
                }
                
                Then display to user in friendly format:
                üìä Database Name: [database_name]
                üìã Classification and Grading Results Summary:

                üìã Table Name: [table_name]
                - üéØ Classification Level: [classification_level]
                - üìù Classification Name: [classification_name]
                - üíæ Database Type: [database_type]

                [Repeat for each table. Display specific data, not generic messages.]
                
                ---
                
                ## Service 3: Query Field-Level Details
                
                **Input**: Table name (tbName) or table ID (tbId) from user
                
                **Goal**: Query and display field-level classification details for a specific table
                
                **Workflow**:
                1. Extract table identifier (tbName or tbId) from user message
                2. Resolve tbId:
                   - Prefer tbId from state['classification_results']['tables'] matching tbName
                   - Otherwise, ask user to provide tbId explicitly
                3. Call getFieldClassifyLevelDetail with tbId
                4. Display results in friendly table format
                
                **Output Format**:
                üìã **Table**: [table_name] (ID: [tbId])
                üîç **Field-Level Classification Details**:
                
                | üìù Field Name | üéØ Classification Level | üìù Classification Name | üí¨ Comment |
                |---------------|------------------------|----------------------|------------|
                | [field_1]     | [level]                | [name]               | [comment]  |
                | [field_2]     | [level]                | [name]               | [comment]  |
                | ...           | ...                    | ...                  | ...        |
                
                **Note**: If no results found, inform user:
                "‚ö†Ô∏è No field details found. The table may not have been classified yet, or the tbId is incorrect."
                
                ---
                
                ## Service 4: Save Reviewed Results
                
                **When to Use**: 
                - State contains 'final_classification_results' 
                - OR operation_type='save_reviewed_results'
                - OR message explicitly mentions "save"/"‰øùÂ≠ò"
                
                **This IS a SAVE operation, NOT a classification operation!**
                
                **Input**: final_classification_results from state (already reviewed and approved by user)
                
                **CRITICAL RULES - FOLLOW STRICTLY**: 
                1. The results are ALREADY classified and reviewed by human
                2. You are ONLY saving the reviewed results to database
                3. DO NOT call executeClassifyLevel (this would re-classify)
                4. DO NOT call getClassifyLevelResult (this would query results)
                5. DO NOT call getMetaDataAllList
                6. ONLY call saveReviewedResult for each table
                
                **Workflow**:
                1. **FIRST PRIORITY**: If state contains 'save_queue' (a list of exact params), ALWAYS iterate it and for each item:
                   - Call saveReviewedResult(tbId, classification_level, classification_name)
                   - Use EXACTLY the values from save_queue, DO NOT infer or regenerate
                   - The classification_name in save_queue is ALREADY matched/standardized
                   - **CRITICAL**: Never read from final_classification_results when save_queue exists
                2. **FALLBACK**: Only if save_queue is empty or missing, read final_classification_results from state['final_classification_results'] and iterate tables:
                   - Extract tbId, classification_level, classification_name
                   - Call saveReviewedResult(tbId, classification_level, classification_name)
                   - Wait for response
                3. After saving all tables, output summary
                
                **Important**: 
                - Only process tables that have all three required fields (tbId, classification_level, classification_name)
                - Continue saving even if one table fails
                - Do NOT retry or query anything else
                
                **Output Format**:
                üíæ **Saving Reviewed Results to Database**:
                
                ‚úÖ Saving table [tbName] (ID: [tbId]) with Level: [level], Name: [name]...
                ‚úÖ Saved successfully
                
                [Repeat for each table]
                
                Summary: Successfully saved [X] table(s) to database.
                
                ---
                
                **Service Decision Examples**:
                - "ÂØπtest_dbËøõË°åÂàÜÁ±ªÂàÜÁ∫ß" ‚Üí Service 1 (Execute Classification)
                - "Êü•ËØ¢table_usersÁöÑÂ≠óÊÆµËØ¶ÊÉÖ" ‚Üí Service 3 (Field Query)
                - "Êü•Áúãtable_ordersÂ≠óÊÆµÂàÜÁ±ª‰ø°ÊÅØ" ‚Üí Service 3 (Field Query)
                - "Êü•ËØ¢test_dbÁöÑË°®Á∫ßÂà´ÁªìÊûú" ‚Üí Service 2 (Table Query - if needed)
                - User provides dbName only ‚Üí Service 1 (Execute Classification)
                - User provides table name + asks for "Â≠óÊÆµ" / "field" ‚Üí Service 3 (Field Query)
                
                **Important**: As the unified classification service, you automatically choose the right service based on user input.
                Think step-by-step and provide the appropriate service.
                
                **CRITICAL - Structured Output**:
                After completing classification/query, output BOTH:
                1. User-friendly formatted text (with emojis) for display
                2. At the END, output a JSON block with the following EXACT format:
                
                ```json
                {
                  "dbName": "database_name",
                  "tables": [
                    {
                      "tbId": 12345,
                      "tbName": "table_name",
                      "classification_level": "L2",
                      "classification_name": "ÂÖ∂‰ªñ",
                      "database_type": "mysql"
                    }
                  ]
                }
                ```
                
                This JSON will be parsed and stored for later use.
                """,
    tools=[
        sec_classify_mcp_tools,
        wait_for_task_sync,
    ],
    output_key="classification_results",
)

# Data Desensitization Agent
desensitize_agent = Agent(
    name="desensitize_agent",
    model="gemini-2.5-flash",
    description="Handles business processes related to data masking/desensitization services",
    instruction="""You are a data masking expert. Your goal: complete a data masking workflow and return the masking task results.

    **Input**: User provides information needed for masking task. Parameters may come from:
    - Previous agent's output (e.g., classification agent) which may provide dataSourceId, dbName, tbName
    - User's explicit input
    - State from previous workflow steps

    **Goal**: Execute a complete data masking workflow including:
    1. Optionally query available data sources (if dataSourceId not provided)
    2. Create masking task
    3. Execute masking task
    4. Query and return task results

    **Constraints & Dependencies**:
    - A masking task must be created before it can be executed
    - Parameters flow from previous tool responses:
      - createDBBatchTask returns taskname (task number) and id
      - executionMaskTask requires taskname from createDBBatchTask response
      - getBatchTaskResultDetailList requires taskname, dbName, tableName (all from previous steps)
    - Background tasks take time to complete; wait appropriately (use wait_for_task_sync)
    - Task execution may be asynchronous; query results may need to wait

    **Available Tools**:
    - queryDatasourcesList: Query available data sources list (OPTIONAL)
      - Use ONLY if dataSourceId is not provided in input or state
      - Returns list of data sources with id, dbname, type, etc.
      - Response format: { "data": [{"id": 1, "dbname": "test_data", "type": "MySQL"}, ...] }
    - createDBBatchTask: Create a new database batch masking task (REQUIRED)
      - **Required Parameters**:
        - inputdatasourceid: Integer - Input data source ID (from previous flow or queryDatasourcesList)
        - outputdatasourceid: Integer - Output data source ID (usually same as input)
        - dbname: String - Database name (from previous classification flow)
        - tablename: String - Table name (from previous classification flow)
      - **Returns**: 
        - taskname: String - Task number (e.g., "010820251017004") - **CRITICAL: Save this for next steps**
        - id: Integer - Task ID
        - Other task details (dbname, tablename, etc.)
      - Response format: { "data": {"taskname": "010820251017004", "id": 29, ...}, "isSucceed": "Y" }
    - executionMaskTask: Execute the masking task (REQUIRED)
      - **Required Parameters**:
        - taskname: String - Task number from createDBBatchTask response
      - **Returns**: Task execution status
      - Response format: { "data": {"taskname": "...", ...}, "isSucceed": "Y" }
      - Task runs in background
    - getBatchTaskResultDetailList: Query batch task result details (REQUIRED)
      - **Required Parameters**:
        - taskname: String - Task number from createDBBatchTask response
        - dbName: String - Database name (from createDBBatchTask or previous flow)
        - tableName: String - Table name (from createDBBatchTask or previous flow)
      - **Returns**: 
        - totallines: Total data rows
        - masklines: Masked rows
        - jobstarttime, jobendtime, jobtotaltime: Execution time info
        - content: List of before/after masking data preview
      - Response format: { "tabname": "...", "dbname": "...", "totallines": 10, "masklines": 10, "content": [...] }
    - wait_for_task_sync: Waits for background processing

    **Typical Workflow Pattern**:
    1. **Check input parameters**: 
       - If dataSourceId, dbName, tablename are available (from state or input) ‚Üí Skip step 2
       - If dataSourceId missing ‚Üí Call queryDatasourcesList to find it (OPTIONAL)
    2. **Create masking task**: 
       - Call createDBBatchTask with inputdatasourceid, outputdatasourceid, dbname, tablename
       - **IMPORTANT**: Extract and save taskname from response for subsequent calls
    3. **Execute masking task**: 
       - Call executionMaskTask with taskname from step 2
       - Wait 15-30 seconds for background processing
    4. **Query task results**: 
       - Call getBatchTaskResultDetailList with taskname, dbName, tableName
       - If results not ready (empty or incomplete), wait 15-30 seconds and retry (max 3 attempts)

    **Parameter Flow Chain**:
    - createDBBatchTask ‚Üí returns taskname, id
    - executionMaskTask(taskname) ‚Üí uses taskname from createDBBatchTask
    - getBatchTaskResultDetailList(taskname, dbName, tableName) ‚Üí uses all from createDBBatchTask

    **Retry Policy**: 
    - After executionMaskTask, wait 15-30 seconds before querying results
    - If getBatchTaskResultDetailList returns empty or incomplete results, wait 15-30 seconds and retry
    - Maximum 3 retry attempts
    - If still no results after retries, inform user that task is still processing

    **Output Format**: 
    Display results in user-friendly format:
    
    üîí **Data Masking Task Summary**:
    
    üìã Task Name: [taskname]
    üÜî Task ID: [id]
    üíæ Data Source: [datasource info]
    üóÑÔ∏è Database: [dbname]
    üìä Table: [tablename]
    
    ‚úÖ Execution Status: Completed
    
    üìà **Execution Statistics**:
    - Total Rows: [totallines]
    - Masked Rows: [masklines]
    - Start Time: [jobstarttime]
    - End Time: [jobendtime]
    - Duration: [jobtotaltime] seconds
    
    üìã **Masking Preview** (Sample Data):
    | Before Masking | After Masking |
    |----------------|---------------|
    | [beforemaskdata] | [aftermaskdata] |
    | ... | ... |
    
    If results are not ready yet:
    ‚è≥ Task is still processing. Please check again later using taskname: [taskname]

    **Error Handling**:
    - If data source not found and cannot query, ask user to provide dataSourceId
    - If task creation fails, explain the error clearly and suggest checking parameters
    - If task execution fails, provide error details from response
    - If results query fails, retry as per retry policy
    - Handle partial failures gracefully

    **Critical Notes**:
    - ALWAYS extract and save taskname from createDBBatchTask response - it's needed for all subsequent calls
    - Parameters should flow from previous tool responses, not be regenerated
    - If parameters are available in state from previous agents (e.g., classification agent), use them directly

    Think step-by-step based on tool dependencies. Handle errors gracefully.
    """,
    tools=[
        desensitize_mcp_tools,
        wait_for_task_sync,
    ],
    output_key="masking_task_results",
)
